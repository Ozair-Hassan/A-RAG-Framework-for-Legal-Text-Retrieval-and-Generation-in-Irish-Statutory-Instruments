{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "This notebook processes the dataset in json format. Each json file has several data samples where a question is mapped to the relevant passages that answer the question accurately and completely. For instance, this would be a sample from the testing dataset.\n",
    "\n",
    "```json\n",
    "  {\n",
    "    \"QuestionID\": \"8d9c9c4a-2a66-4d1e-8b0b-0bbf3e3f1c2e\",\n",
    "    \"Question\": \"When does a person who purchases a dwelling on or after the commencement of these Regulations cease to qualify as a “relevant owner” for the scheme?\",\n",
    "    \"Passages\": [\n",
    "      {\n",
    "        \"DocumentID\": \"si-2020-0025\",\n",
    "        \"PassageID\": \"reg-5\",\n",
    "        \"Passage\": \"(2) Where a person purchases a dwelling on or after the date of the coming \\ninto operation of these Regulations, he or she shall not be a relevant owner for \\n\\n \\n \\n \\n\\f[25] 7 \\n\\nthe purposes of these Regulations where he or she knew, or ought to have \\nknown, that defective concrete blocks were used in the construction of the \\ndwelling.\"\n",
    "      }\n",
    "    ],\n",
    "    \"Group\": 1\n",
    "  },\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Data handling\n",
    "from datasets import Dataset\n",
    "from pandas import to_pickle\n",
    "\n",
    "# Other utils\n",
    "import json\n",
    "from re import compile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform some basic clean up of the query\n",
    "def simple_cleaning(query: str) -> str:\n",
    "    pattern_newline = compile(r'[\\n\\t\\u200e]')  # Remove new lines, tabs, and undesired characters\n",
    "    pattern_multiple_spaces = compile(r' +')  # Remove contiguous blank spaces\n",
    "\n",
    "    cln_query = pattern_newline.sub(' ', query)\n",
    "    cln_query = pattern_multiple_spaces.sub(' ', cln_query).strip()\n",
    "    return cln_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2865, Eval: 614, Test: 615\n"
     ]
    }
   ],
   "source": [
    "with open('../all_data.json', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# Split into train (70%), eval (15%), test (15%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, temp = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "data_eval, data_test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(data_train)}, Eval: {len(data_eval)}, Test: {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2865, 614, 615)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train), len(data_eval), len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus\n",
    "\n",
    "Creates a corpus pickle from the list of 40 regulatory documents and saves it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../all_data.json', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "collection = []\n",
    "seen = set()\n",
    "\n",
    "for q in all_data:\n",
    "    for psg in q['Passages']:\n",
    "        psg_id = f\"{psg['DocumentID']}-{psg['PassageID']}\"\n",
    "        if psg_id not in seen:\n",
    "            passage_text = psg['PassageID'] + \" \" + psg['Passage']\n",
    "            if len(passage_text) > 100:\n",
    "                collection.append({\n",
    "                    'text': passage_text,\n",
    "                    'ID': psg_id,\n",
    "                    'DocumentId': psg['DocumentID'],\n",
    "                    'PassageId': psg['PassageID'],\n",
    "                })\n",
    "                seen.add(psg_id)\n",
    "                \n",
    "corpus = {f\"{doc['DocumentId']}-{doc['PassageId']}\": doc[\"text\"] for doc in collection}\n",
    "# Save the corpus to disk\n",
    "to_pickle(corpus, './data/corpus.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: reg-1 1. (1) These Regulations may be cited as the Betting Duty and Betting \n",
      "\n",
      "Intermediary Duty (Amendment) Regulations 2020. \n",
      "\n",
      "(2) These Regulations come into operation with immediate effect. \n",
      "\n",
      "Interpretation\n"
     ]
    }
   ],
   "source": [
    "# Quick check:\n",
    "import pickle\n",
    "with open('./data/corpus.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "first_key = list(corpus.keys())[0]\n",
    "print(\"Sample:\", corpus[first_key][:2000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

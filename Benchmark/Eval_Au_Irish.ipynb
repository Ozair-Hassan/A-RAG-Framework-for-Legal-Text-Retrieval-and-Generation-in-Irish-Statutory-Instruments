{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulatory Information Retrieval and Answer Generation\n",
    "\n",
    "This notebook runs the original approach from \"A Hybrid Approach To Information Retrieval And Answer Generation For Regulatory Texts\" with minimal adjustments to run on Irish S.I dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "from re import compile\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import fix as fix_contractions\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.feature_extraction import text as sk_text\n",
    "from trectools import TrecRun, TrecQrel, TrecEval # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "Kept original from \"A Hybrid Approach To Information Retrieval And Answer Generation For Regulatory Texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RSVrz2DPD4Sz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32810 passages into collection\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(docs_dir: str, fqrels: str) -> Dict[str, Dict[str, int]]:\n",
    "    with open(fqrels, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    qrels = {}\n",
    "    \n",
    "    for e in data:\n",
    "        qid = e[\"QuestionID\"]\n",
    "        for psg in e[\"Passages\"]:\n",
    "            qrels.setdefault(qid, {})\n",
    "            pid = f\"{psg['DocumentID']}-{psg['PassageID']}\"\n",
    "            qrels[qid][pid] = 1\n",
    "\n",
    "    return qrels\n",
    "\n",
    "file_type = 'test'\n",
    "qrels = load_qrels(\"\", \"./QnA_complete.json\")\n",
    "\n",
    "with open(\"./data/qrels\", \"w\") as f:\n",
    "    for qid, rels in qrels.items():\n",
    "        for pid, rel in rels.items():\n",
    "            line = f\"{qid} Q0 {pid} {rel}\"\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "with open('../all_data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "collection = []\n",
    "seen = set()\n",
    "\n",
    "for doc in all_data:\n",
    "    for psg in doc['Passages']:\n",
    "        psg_id = f\"{psg['DocumentID']}-{psg['PassageID']}\"\n",
    "        if psg_id not in seen:\n",
    "            passage_text = psg['PassageID'] + \" \" + psg['Passage']\n",
    "            if len(passage_text) > 100:\n",
    "                collection.append(\n",
    "                    dict(\n",
    "                        text=passage_text,\n",
    "                        ID=psg_id,\n",
    "                        DocumentId=psg['DocumentID'],\n",
    "                        PassageId=psg['PassageID'],\n",
    "                    )\n",
    "                )\n",
    "                seen.add(psg_id)\n",
    "\n",
    "print(f\"Loaded {len(collection)} passages into collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = sk_text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "pattern_newline = compile(r'[\\n\\t\\u200e]')\n",
    "pattern_multiple_spaces = compile(r' +')\n",
    "pattern_non_alphanumeric = compile(r'[^a-z0-9]')\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    cln_text = fix_contractions(text)\n",
    "    \n",
    "    cln_text = cln_text.lower()\n",
    "    \n",
    "    cln_text = pattern_newline.sub(' ', cln_text)\n",
    "    \n",
    "    cln_text = pattern_non_alphanumeric.sub(' ', cln_text)\n",
    "    \n",
    "    tokens = [stemmer.stem(word) for word in word_tokenize(cln_text) if word not in stop_words]\n",
    "    \n",
    "    cln_text = ' '.join(tokens)\n",
    "    \n",
    "    cln_text = pattern_multiple_spaces.sub(' ', cln_text).strip()\n",
    "    \n",
    "    return cln_text\n",
    "\n",
    "def simple_cleaning(query: str) -> str:\n",
    "    \n",
    "    cln_query = pattern_newline.sub(' ', query)\n",
    "    cln_query = pattern_multiple_spaces.sub(' ', cln_query).strip()\n",
    "    return cln_query\n",
    "\n",
    "def tokenizer(text:str)-> list:\n",
    "\n",
    "    tokens = text.split()\n",
    "    \n",
    "    unigrams = tokens\n",
    "    \n",
    "    bigrams = [f\"{tokens[i]} {tokens[i + 1]}\" for i in range(len(tokens) - 1)]\n",
    "    \n",
    "    return unigrams + bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [tokenizer(clean_text(doc['text'])) for doc in collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32810"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_array = np.array(collection)\n",
    "\n",
    "len(tokenized_corpus) # 10592 (originalmente 13732)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Retriever: BM25\n",
    "\n",
    "Let us evaluate the lexical retriever using BM25 (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sintactic_query_bm5(query: str, bm5_instance: BM25Okapi) -> np.array:\n",
    "\n",
    "    tokenized_query = tokenizer(clean_text(query))\n",
    "    \n",
    "    scores = bm5_instance.get_scores(tokenized_query)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintactic_bm25_retriever = partial(sintactic_query_bm5, bm5_instance=bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:44<00:00,  5.35it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved = {}\n",
    "top_n = 20\n",
    "\n",
    "with open(\"./QnA_complete.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for e in tqdm(data):\n",
    "        query = e['Question']\n",
    "        \n",
    "        scores = sintactic_bm25_retriever(query)\n",
    "        \n",
    "        top_k = np.argpartition(-scores, top_n)[:top_n]\n",
    "        \n",
    "        top_k = top_k[np.argsort(-scores[top_k])]\n",
    "\n",
    "        top_docs = collection_array[top_k]\n",
    "\n",
    "        top_scores = scores[top_k]\n",
    "\n",
    "        top_results = [{**doc, 'score': score} for doc, score in zip(top_docs, top_scores)]\n",
    "\n",
    "        retrieved[e[\"QuestionID\"]] = top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/rankings_sintactic.trec\", \"w\") as f:\n",
    "    for qid, hits in retrieved.items():\n",
    "        for i, hit in enumerate(hits):\n",
    "            line = f\"{qid} 0 {hit['ID']} {i+1} {hit['score']} bm25\"\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25_recall_10             \tall\t0.4875\n",
      "bm25_map_cut_10            \tall\t0.2991\n"
     ]
    }
   ],
   "source": [
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_sintactic.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "bm25_recall_10 = te.get_recall(depth=10)\n",
    "bm25_map_cut_10 = te.get_map(depth=10)\n",
    "\n",
    "print(f\"bm25_recall_10             \\tall\\t{bm25_recall_10:.4f}\")\n",
    "print(f\"bm25_map_cut_10            \\tall\\t{bm25_map_cut_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25_recall_20             \tall\t0.5500\n",
      "bm25_map_cut_20            \tall\t0.3034\n"
     ]
    }
   ],
   "source": [
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_sintactic.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "bm25_recall_20 = te.get_recall(depth=20)\n",
    "bm25_map_cut_20 = te.get_map(depth=20)\n",
    "\n",
    "print(f\"bm25_recall_20             \\tall\\t{bm25_recall_20:.4f}\")\n",
    "print(f\"bm25_map_cut_20            \\tall\\t{bm25_map_cut_20:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"bm25_recall_10\": bm25_recall_10,\n",
    "    \"bm25_map_cut_10\": bm25_map_cut_10,\n",
    "    \"bm25_recall_20\": bm25_recall_20,\n",
    "    \"bm25_map_cut_20\": bm25_map_cut_20\n",
    "}\n",
    "\n",
    "retriever_name = \"BM25-Baseline\"  # Hybrid / Vector\n",
    "\n",
    "names = list(metrics.keys())\n",
    "values = list(metrics.values())\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(names, values)\n",
    "plt.ylabel(\"score\")\n",
    "plt.xlabel(\"metric\")\n",
    "plt.title(f\"{retriever_name} Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{retriever_name}_metrics.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Retriever: Fine Tunned BAAI/bge-small-en-v1.5\n",
    "\n",
    "Vector retriever using a fine-tuned model based on `BAAI/bge-small-en-v1.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = SentenceTransformer(\n",
    "    'raul-delarosa99/bge-small-en-v1.5-RIRAG_ObliQA',\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_query(query: str, corpus_embeddings_matrix: np.array, \n",
    "                   sentence_transformer_model: SentenceTransformer) -> np.array:\n",
    "    query_emb = sentence_transformer_model.encode([simple_cleaning(query)], \n",
    "                                                  device='cuda',\n",
    "                                                  normalize_embeddings=True)\n",
    "    scores = (query_emb @ corpus_embeddings_matrix.T)[0]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bcb0d5febe4f48b2b06148539b04e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_embeddings_matrix = sentence_transformer_model.encode([simple_cleaning(doc['text']) for doc in collection_array],\n",
    "                          normalize_embeddings=True,\n",
    "                          show_progress_bar=True,\n",
    "                          max_length=512,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_retriever = partial(semantic_query, corpus_embeddings_matrix=corpus_embeddings_matrix,\n",
    "                             sentence_transformer_model=sentence_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:03<00:00, 66.10it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved = {}\n",
    "top_n = 20\n",
    "\n",
    "with open(\"./QnA_complete.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for e in tqdm(data):\n",
    "        query = e['Question']\n",
    "        \n",
    "        scores = semantic_retriever(query)\n",
    "        \n",
    "        top_k = np.argpartition(-scores, top_n)[:top_n]\n",
    "        \n",
    "        top_k = top_k[np.argsort(-scores[top_k])]\n",
    "\n",
    "        top_docs = collection_array[top_k]\n",
    "\n",
    "        top_scores = scores[top_k]\n",
    "\n",
    "        top_results = [{**doc, 'score': score} for doc, score in zip(top_docs, top_scores)]\n",
    "        \n",
    "        retrieved[e[\"QuestionID\"]] = top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/rankings_semantic.trec\", \"w\") as f:\n",
    "    for qid, hits in retrieved.items():\n",
    "        for i, hit in enumerate(hits): \n",
    "            line = f\"{qid} 0 {hit['ID']} {i+1} {hit['score']} dense\"\n",
    "            f.write(line + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_recall_10             \tall\t0.4458\n",
      "vector_map_cut_10            \tall\t0.2529\n"
     ]
    }
   ],
   "source": [
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_semantic.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "vector_recall_10 = te.get_recall(depth=10)\n",
    "vector_map_cut_10 = te.get_map(depth=10)\n",
    "\n",
    "print(f\"vector_recall_10             \\tall\\t{vector_recall_10:.4f}\")\n",
    "print(f\"vector_map_cut_10            \\tall\\t{vector_map_cut_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_recall_20             \tall\t0.5000\n",
      "vector_map_cut_20            \tall\t0.2564\n"
     ]
    }
   ],
   "source": [
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_semantic.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "vector_recall_20 = te.get_recall(depth=20)\n",
    "vector_map_cut_20 = te.get_map(depth=20)\n",
    "\n",
    "print(f\"vector_recall_20             \\tall\\t{vector_recall_20:.4f}\")\n",
    "print(f\"vector_map_cut_20            \\tall\\t{vector_map_cut_20:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"vector_recall_10\": vector_recall_10,\n",
    "    \"vector_map_cut_10\": vector_map_cut_10,\n",
    "    \"vector_recall_20\": vector_recall_20,\n",
    "    \"vector_map_cut_20\": vector_map_cut_20\n",
    "}\n",
    "\n",
    "retriever_name = \"Vector-Baseline\"  # Hybrid / BM25\n",
    "\n",
    "names = list(metrics.keys())\n",
    "values = list(metrics.values())\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(names, values)\n",
    "plt.ylabel(\"score\")\n",
    "plt.xlabel(\"metric\")\n",
    "plt.title(f\"{retriever_name} Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{retriever_name}_metrics.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Retriever (BM25 + Fine Tunned BAAI/bge-small-en-v1.5)\n",
    "\n",
    "Hybrid retriever using a fine-tuned model based on `BAAI/bge-small-en-v1.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_query_avg(query: str, sintactic_retriever: partial, semantic_retriever: partial, \n",
    "                     alpha: float = 0.5) -> np.array:    \n",
    "    \n",
    "    sintactic_scores = sintactic_retriever(query)\n",
    "    sintactic_scores = (sintactic_scores - sintactic_scores.min()) / (sintactic_scores.max() - sintactic_scores.min())\n",
    "    \n",
    "    semantic_scores = semantic_retriever(query)\n",
    "    semantic_scores = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min())\n",
    "    \n",
    "    scores = alpha * semantic_scores + (1 - alpha) * sintactic_scores\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:55<00:00,  4.33it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved = {}\n",
    "top_n = 20\n",
    "\n",
    "with open(\"./QnA_complete.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for e in tqdm(data):\n",
    "        query = e['Question']\n",
    "        \n",
    "        scores = hybrid_query_avg(\n",
    "                                query,\n",
    "                                sintactic_retriever=sintactic_bm25_retriever,\n",
    "                                semantic_retriever=semantic_retriever,\n",
    "                                alpha=0.65\n",
    "                                )\n",
    "        \n",
    "        top_k = np.argpartition(-scores, top_n)[:top_n]\n",
    "\n",
    "        top_k = top_k[np.argsort(-scores[top_k])]\n",
    "\n",
    "        top_docs = collection_array[top_k]\n",
    "\n",
    "        top_scores = scores[top_k]\n",
    "\n",
    "        top_results = [{**doc, 'score': score} for doc, score in zip(top_docs, top_scores)]\n",
    "        \n",
    "        retrieved[e[\"QuestionID\"]] = top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/rankings_hybrid.trec\", \"w\") as f:\n",
    "    for qid, hits in retrieved.items():\n",
    "        for i, hit in enumerate(hits):\n",
    "            line = f\"{qid} 0 {hit['ID']} {i+1} {hit['score']} hybrid\"\n",
    "            f.write(line + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVWcEMRRQfWZ",
    "outputId": "8ec08829-4340-4167-8e0d-fe6c9de74505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid_recall_10             \tall\t0.4958\n",
      "hybrid_map_cut_10            \tall\t0.2989\n"
     ]
    }
   ],
   "source": [
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_hybrid.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "hybrid_recall_10 = te.get_recall(depth=10)\n",
    "hybrid_map_cut_10 = te.get_map(depth=10)\n",
    "\n",
    "print(f\"hybrid_recall_10             \\tall\\t{hybrid_recall_10:.4f}\")\n",
    "print(f\"hybrid_map_cut_10            \\tall\\t{hybrid_map_cut_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid_recall_20             \tall\t0.5625\n",
      "hybrid_map_cut_20            \tall\t0.3038\n"
     ]
    }
   ],
   "source": [
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_hybrid.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "hybrid_recall_20 = te.get_recall(depth=20)\n",
    "hybrid_map_cut_20 = te.get_map(depth=20)\n",
    "\n",
    "print(f\"hybrid_recall_20             \\tall\\t{hybrid_recall_20:.4f}\")\n",
    "print(f\"hybrid_map_cut_20            \\tall\\t{hybrid_map_cut_20:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"hybrid_recall_10\": hybrid_recall_10,\n",
    "    \"hybrid_map_cut_10\": hybrid_map_cut_10,\n",
    "    \"hybrid_recall_20\": hybrid_recall_20,\n",
    "    \"hybrid_map_cut_20\": hybrid_map_cut_20\n",
    "}\n",
    "\n",
    "retriever_name = \"Hybrid-Baseline\"  # Vector / BM25\n",
    "\n",
    "names = list(metrics.keys())\n",
    "values = list(metrics.values())\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(names, values)\n",
    "plt.ylabel(\"score\")\n",
    "plt.xlabel(\"metric\")\n",
    "plt.title(f\"{retriever_name} Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{retriever_name}_metrics.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "retrievers = [\"bm25\", \"vector\", \"hybrid\"]\n",
    "x = np.arange(len(retrievers))\n",
    "width = 0.35\n",
    "\n",
    "recall_10_vals = [bm25_recall_10, vector_recall_10, hybrid_recall_10]\n",
    "recall_20_vals = [bm25_recall_20, vector_recall_20, hybrid_recall_20]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(x - width/2, recall_10_vals, width, label=\"Recall@10\")\n",
    "plt.bar(x + width/2, recall_20_vals, width, label=\"Recall@20\")\n",
    "plt.xticks(x, retrievers)\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.xlabel(\"retriever\")\n",
    "plt.title(\"Recall Comparison\")\n",
    "plt.ylim(0, 0.7)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for i, v in enumerate(recall_10_vals):\n",
    "    plt.text(x[i] - width/2, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "for i, v in enumerate(recall_20_vals):\n",
    "    plt.text(x[i] + width/2, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_recall.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "retrievers = [\"bm25\", \"vector\", \"hybrid\"]\n",
    "x = np.arange(len(retrievers))\n",
    "width = 0.35\n",
    "\n",
    "map_10_vals = [bm25_map_cut_10, vector_map_cut_10, hybrid_map_cut_10]\n",
    "map_20_vals = [bm25_map_cut_20, vector_map_cut_20, hybrid_map_cut_20]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(x - width/2, map_10_vals, width, label=\"MAP@10\")\n",
    "plt.bar(x + width/2, map_20_vals, width, label=\"MAP@20\")\n",
    "plt.xticks(x, retrievers)\n",
    "plt.ylabel(\"MAP\")\n",
    "plt.xlabel(\"retriever\")\n",
    "plt.title(\"MAP Comparison\")\n",
    "plt.ylim(0, 0.5)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for i, v in enumerate(map_10_vals):\n",
    "    plt.text(x[i] - width/2, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "for i, v in enumerate(map_20_vals):\n",
    "    plt.text(x[i] + width/2, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_map.png\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

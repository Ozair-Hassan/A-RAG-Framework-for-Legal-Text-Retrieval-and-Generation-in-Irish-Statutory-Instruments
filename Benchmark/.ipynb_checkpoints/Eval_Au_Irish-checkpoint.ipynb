{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulatory Information Retrieval and Answer Generation\n",
    "\n",
    "This notebook runs the original approach from \"A Hybrid Approach To Information Retrieval And Answer Generation For Regulatory Texts\" with minimal adjustments to run on Irish S.I dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy trec_eval repo to validate our approach - ONLY RUN ONCE\n",
    "#!git clone https://github.com/usnistgov/trec_eval.git && cd trec_eval && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "from re import compile\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import fix as fix_contractions\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.feature_extraction import text as sk_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "Kept original from \"A Hybrid Approach To Information Retrieval And Answer Generation For Regulatory Texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "RSVrz2DPD4Sz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32810 passages into collection\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(docs_dir: str, fqrels: str) -> Dict[str, Dict[str, int]]:\n",
    "    with open(fqrels, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    qrels = {}\n",
    "    \n",
    "    for e in data:\n",
    "        qid = e[\"QuestionID\"]\n",
    "        for psg in e[\"Passages\"]:\n",
    "            qrels.setdefault(qid, {})\n",
    "            pid = f\"{psg['DocumentID']}-{psg['PassageID']}\"\n",
    "            qrels[qid][pid] = 1\n",
    "\n",
    "    return qrels\n",
    "\n",
    "file_type = 'test'\n",
    "qrels = load_qrels(\"\", \"./QnA_complete_fixed.json\")\n",
    "\n",
    "with open(\"./data/qrels\", \"w\") as f:\n",
    "    for qid, rels in qrels.items():\n",
    "        for pid, rel in rels.items():\n",
    "            line = f\"{qid} Q0 {pid} {rel}\"\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "with open('../../all_data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "collection = []\n",
    "seen = set()\n",
    "\n",
    "for doc in all_data:\n",
    "    for psg in doc['Passages']:\n",
    "        psg_id = f\"{psg['DocumentID']}-{psg['PassageID']}\"\n",
    "        if psg_id not in seen:\n",
    "            passage_text = psg['PassageID'] + \" \" + psg['Passage']\n",
    "            if len(passage_text) > 100:\n",
    "                collection.append(\n",
    "                    dict(\n",
    "                        text=passage_text,\n",
    "                        ID=psg_id,\n",
    "                        DocumentId=psg['DocumentID'],\n",
    "                        PassageId=psg['PassageID'],\n",
    "                    )\n",
    "                )\n",
    "                seen.add(psg_id)\n",
    "\n",
    "print(f\"Loaded {len(collection)} passages into collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = sk_text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "pattern_newline = compile(r'[\\n\\t\\u200e]')\n",
    "pattern_multiple_spaces = compile(r' +')\n",
    "pattern_non_alphanumeric = compile(r'[^a-z0-9]')\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    cln_text = fix_contractions(text)\n",
    "    \n",
    "    cln_text = cln_text.lower()\n",
    "    \n",
    "    cln_text = pattern_newline.sub(' ', cln_text)\n",
    "    \n",
    "    cln_text = pattern_non_alphanumeric.sub(' ', cln_text)\n",
    "    \n",
    "    tokens = [stemmer.stem(word) for word in word_tokenize(cln_text) if word not in stop_words]\n",
    "    \n",
    "    cln_text = ' '.join(tokens)\n",
    "    \n",
    "    cln_text = pattern_multiple_spaces.sub(' ', cln_text).strip()\n",
    "    \n",
    "    return cln_text\n",
    "\n",
    "def simple_cleaning(query: str) -> str:\n",
    "    \n",
    "    cln_query = pattern_newline.sub(' ', query)\n",
    "    cln_query = pattern_multiple_spaces.sub(' ', cln_query).strip()\n",
    "    return cln_query\n",
    "\n",
    "def tokenizer(text:str)-> list:\n",
    "\n",
    "    tokens = text.split()\n",
    "    \n",
    "    unigrams = tokens\n",
    "    \n",
    "    bigrams = [f\"{tokens[i]} {tokens[i + 1]}\" for i in range(len(tokens) - 1)]\n",
    "    \n",
    "    return unigrams + bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [tokenizer(clean_text(doc['text'])) for doc in collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32810"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_array = np.array(collection)\n",
    "\n",
    "len(tokenized_corpus) # 10592 (originalmente 13732)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Retriever: BM25\n",
    "\n",
    "Let us evaluate the lexical retriever using BM25 (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sintactic_query_bm5(query: str, bm5_instance: BM25Okapi) -> np.array:\n",
    "\n",
    "    tokenized_query = tokenizer(clean_text(query))\n",
    "    \n",
    "    scores = bm5_instance.get_scores(tokenized_query)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintactic_bm25_retriever = partial(sintactic_query_bm5, bm5_instance=bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:06<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved = {}\n",
    "top_n = 20\n",
    "\n",
    "with open(\"./QnA_complete_fixed.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for e in tqdm(data):\n",
    "        query = e['Question']\n",
    "        \n",
    "        scores = sintactic_bm25_retriever(query)\n",
    "        \n",
    "        top_k = np.argpartition(-scores, top_n)[:top_n]\n",
    "        \n",
    "        top_k = top_k[np.argsort(-scores[top_k])]\n",
    "\n",
    "        top_docs = collection_array[top_k]\n",
    "\n",
    "        top_scores = scores[top_k]\n",
    "\n",
    "        top_results = [{**doc, 'score': score} for doc, score in zip(top_docs, top_scores)]\n",
    "\n",
    "        retrieved[e[\"QuestionID\"]] = top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/rankings_sintactic.trec\", \"w\") as f:\n",
    "    for qid, hits in retrieved.items():\n",
    "        for i, hit in enumerate(hits):\n",
    "            line = f\"{qid} 0 {hit['ID']} {i+1} {hit['score']} bm25\"\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_10             \tall\t0.4875\n",
      "map_cut_10            \tall\t0.2991\n"
     ]
    }
   ],
   "source": [
    "from trectools import TrecRun, TrecQrel, TrecEval # type: ignore\n",
    "\n",
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_sintactic.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "recall_10 = te.get_recall(depth=10)\n",
    "map_10 = te.get_map(depth=10)\n",
    "\n",
    "print(f\"recall_10             \\tall\\t{recall_10:.4f}\")\n",
    "print(f\"map_cut_10            \\tall\\t{map_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_20             \tall\t0.5500\n",
      "map_cut_20            \tall\t0.3034\n"
     ]
    }
   ],
   "source": [
    "from trectools import TrecRun, TrecQrel, TrecEval # type: ignore\n",
    "\n",
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_sintactic.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "recall_20 = te.get_recall(depth=20)\n",
    "map_20 = te.get_map(depth=20)\n",
    "\n",
    "print(f\"recall_20             \\tall\\t{recall_20:.4f}\")\n",
    "print(f\"map_cut_20            \\tall\\t{map_20:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Retriever: Fine Tunned BAAI/bge-small-en-v1.5\n",
    "\n",
    "Semantic retriever using a fine-tuned model based on `BAAI/bge-small-en-v1.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = SentenceTransformer(\n",
    "    'raul-delarosa99/bge-small-en-v1.5-RIRAG_ObliQA',\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_query(query: str, corpus_embeddings_matrix: np.array, \n",
    "                   sentence_transformer_model: SentenceTransformer) -> np.array:\n",
    "    query_emb = sentence_transformer_model.encode([simple_cleaning(query)], \n",
    "                                                  device='cuda',\n",
    "                                                  normalize_embeddings=True)\n",
    "    scores = (query_emb @ corpus_embeddings_matrix.T)[0]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4c44ab372641b38f29d6813fd3b4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_embeddings_matrix = sentence_transformer_model.encode([simple_cleaning(doc['text']) for doc in collection_array],\n",
    "                          normalize_embeddings=True,\n",
    "                          show_progress_bar=True,\n",
    "                          max_length=512,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_retriever = partial(semantic_query, corpus_embeddings_matrix=corpus_embeddings_matrix,\n",
    "                             sentence_transformer_model=sentence_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:04<00:00, 56.88it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved = {}\n",
    "top_n = 20\n",
    "\n",
    "with open(\"./QnA_complete_fixed.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for e in tqdm(data):\n",
    "        query = e['Question']\n",
    "        \n",
    "        scores = semantic_retriever(query)\n",
    "        \n",
    "        top_k = np.argpartition(-scores, top_n)[:top_n]\n",
    "        \n",
    "        top_k = top_k[np.argsort(-scores[top_k])]\n",
    "\n",
    "        top_docs = collection_array[top_k]\n",
    "\n",
    "        top_scores = scores[top_k]\n",
    "\n",
    "        top_results = [{**doc, 'score': score} for doc, score in zip(top_docs, top_scores)]\n",
    "        \n",
    "        retrieved[e[\"QuestionID\"]] = top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/rankings_semantic.trec\", \"w\") as f:\n",
    "    for qid, hits in retrieved.items():\n",
    "        for i, hit in enumerate(hits): \n",
    "            line = f\"{qid} 0 {hit['ID']} {i+1} {hit['score']} dense\"\n",
    "            f.write(line + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_10             \tall\t0.4458\n",
      "map_cut_10            \tall\t0.2529\n"
     ]
    }
   ],
   "source": [
    "from trectools import TrecRun, TrecQrel, TrecEval\n",
    "\n",
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_semantic.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "recall_10 = te.get_recall(depth=10)\n",
    "map_10 = te.get_map(depth=10)\n",
    "\n",
    "print(f\"recall_10             \\tall\\t{recall_10:.4f}\")\n",
    "print(f\"map_cut_10            \\tall\\t{map_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_20             \tall\t0.5000\n",
      "map_cut_20            \tall\t0.2564\n"
     ]
    }
   ],
   "source": [
    "from trectools import TrecRun, TrecQrel, TrecEval\n",
    "\n",
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_semantic.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "recall_20 = te.get_recall(depth=20)\n",
    "map_20 = te.get_map(depth=20)\n",
    "\n",
    "print(f\"recall_20             \\tall\\t{recall_20:.4f}\")\n",
    "print(f\"map_cut_20            \\tall\\t{map_20:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Retriever (BM25 + Fine Tunned BAAI/bge-small-en-v1.5)\n",
    "\n",
    "Hybrid retriever using a fine-tuned model based on `BAAI/bge-small-en-v1.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_query_avg(query: str, sintactic_retriever: partial, semantic_retriever: partial, \n",
    "                     alpha: float = 0.5) -> np.array:    \n",
    "    \n",
    "    sintactic_scores = sintactic_retriever(query)\n",
    "    sintactic_scores = (sintactic_scores - sintactic_scores.min()) / (sintactic_scores.max() - sintactic_scores.min())\n",
    "    \n",
    "    semantic_scores = semantic_retriever(query)\n",
    "    semantic_scores = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min())\n",
    "    \n",
    "    scores = alpha * semantic_scores + (1 - alpha) * sintactic_scores\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:15<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved = {}\n",
    "top_n = 20\n",
    "\n",
    "with open(\"./QnA_complete_fixed.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for e in tqdm(data):\n",
    "        query = e['Question']\n",
    "        \n",
    "        scores = hybrid_query_avg(\n",
    "                                query,\n",
    "                                sintactic_retriever=sintactic_bm25_retriever,\n",
    "                                semantic_retriever=semantic_retriever,\n",
    "                                alpha=0.65\n",
    "                                )\n",
    "        \n",
    "        top_k = np.argpartition(-scores, top_n)[:top_n]\n",
    "\n",
    "        top_k = top_k[np.argsort(-scores[top_k])]\n",
    "\n",
    "        top_docs = collection_array[top_k]\n",
    "\n",
    "        top_scores = scores[top_k]\n",
    "\n",
    "        top_results = [{**doc, 'score': score} for doc, score in zip(top_docs, top_scores)]\n",
    "        \n",
    "        retrieved[e[\"QuestionID\"]] = top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/rankings_hybrid.trec\", \"w\") as f:\n",
    "    for qid, hits in retrieved.items():\n",
    "        for i, hit in enumerate(hits):\n",
    "            line = f\"{qid} 0 {hit['ID']} {i+1} {hit['score']} hybrid\"\n",
    "            f.write(line + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVWcEMRRQfWZ",
    "outputId": "8ec08829-4340-4167-8e0d-fe6c9de74505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_10             \tall\t0.4958\n",
      "map_cut_10            \tall\t0.2989\n"
     ]
    }
   ],
   "source": [
    "from trectools import TrecRun, TrecQrel, TrecEval # type: ignore\n",
    "\n",
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_hybrid.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "recall_10 = te.get_recall(depth=10)\n",
    "map_10 = te.get_map(depth=10)\n",
    "\n",
    "print(f\"recall_10             \\tall\\t{recall_10:.4f}\")\n",
    "print(f\"map_cut_10            \\tall\\t{map_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_20             \tall\t0.5625\n",
      "map_cut_20            \tall\t0.3038\n"
     ]
    }
   ],
   "source": [
    "from trectools import TrecRun, TrecQrel, TrecEval # type: ignore\n",
    "\n",
    "qrels = TrecQrel(\"./data/qrels\")\n",
    "run = TrecRun(\"./data/rankings_hybrid.trec\")\n",
    "te = TrecEval(run, qrels)\n",
    "\n",
    "recall_20 = te.get_recall(depth=20)\n",
    "map_20 = te.get_map(depth=20)\n",
    "\n",
    "print(f\"recall_20             \\tall\\t{recall_20:.4f}\")\n",
    "print(f\"map_cut_20            \\tall\\t{map_20:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
